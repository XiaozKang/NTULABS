{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7773a852ff5dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T12:21:28.721657Z",
     "start_time": "2023-12-27T12:21:27.436457Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets tiktoken tqdm wandb numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac0721850d50ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T08:23:31.964998Z",
     "start_time": "2023-12-24T08:23:08.997466Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install  --pre torch torchvision torchaudio   --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28149ddddc9d285c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T08:56:52.838376Z",
     "start_time": "2023-12-24T08:56:51.380274Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Import packages! \n",
    "import os,pickle,requests,time,math,torch,tiktoken\n",
    "import numpy as np\n",
    "from contextlib import nullcontext\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a27fb8c90d84bdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T08:56:53.060266Z",
     "start_time": "2023-12-24T08:56:52.983354Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Download the tiny shakespeare dataset!\n",
    "input_file_path = './data/shakespeare_char/input.txt'\n",
    "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "if not os.path.exists(input_file_path):\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] \n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) \n",
    "\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('./data/shakespeare_char/train.bin')\n",
    "val_ids.tofile('./data/shakespeare_char/val.bin')\n",
    "\n",
    "meta = {\n",
    "            'vocab_size': vocab_size,\n",
    "            'itos': itos,\n",
    "            'stoi': stoi,\n",
    "            }\n",
    "with open('./data/shakespeare_char/meta.pkl', 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63454596d0ae37b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T09:08:47.829709Z",
     "start_time": "2023-12-24T09:08:47.821400Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Configuration\n",
    "dataset = 'shakespeare_char'\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "out_dir = './out'\n",
    "eval_interval = 200\n",
    "eval_iters = 1\n",
    "batch_size = 12\n",
    "block_size = 64\n",
    "learning_rate = 6e-4\n",
    "max_iters = 5000\n",
    "decay_lr = True\n",
    "warmup_iters = 100\n",
    "lr_decay_iters = max_iters\n",
    "min_lr = learning_rate/10.0\n",
    "tokens_per_iter = batch_size * block_size\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "device = 'mps'\n",
    "device_type = 'cpu'\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype='float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd504dca2334c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T09:08:48.722001Z",
     "start_time": "2023-12-24T09:08:48.718431Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Define some functions \n",
    "\n",
    "# NOTE:++++++++++++++\n",
    "# NOTE: Build a batch loader\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# NOTE:++++++++++++++\n",
    "# NOTE: Build a loss estimator\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# NOTE:++++++++++++++\n",
    "# NOTE: Build a learning rate adjustor\n",
    "def get_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d865721de9f8e83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T09:08:49.526872Z",
     "start_time": "2023-12-24T09:08:49.500967Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Build a model and a optimizer\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "model_args = dict(n_layer=4, n_head=4, n_embd=128, block_size=block_size, bias=False, vocab_size=None, dropout=0.0) \n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "model.to(device)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=False)\n",
    "optimizer = model.configure_optimizers(1e-1, learning_rate, (0.9, 0.95), device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f0240e70cfe2d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T09:11:14.378530Z",
     "start_time": "2023-12-24T09:08:50.454837Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Training phase begins\n",
    "if not os.path.exists(out_dir): \n",
    "    print(\"make dir=>{}\".format(out_dir))\n",
    "    os.makedirs(out_dir)\n",
    "t0 = time.time()\n",
    "X, Y = get_batch('train')\n",
    "while True:\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    if iter_num == 0: \n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model_args': model_args,\n",
    "            'iter_num': iter_num,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            }\n",
    "        print(f\"saving initial checkpoint to {out_dir}\")\n",
    "        torch.save(checkpoint, os.path.join(out_dir, 'ckpt_init.pt'))\n",
    "    \n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(iter_num % eval_interval)\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    for micro_step in range(1):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss\n",
    "\n",
    "        X, Y = get_batch('train')\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % 1 == 0:\n",
    "        lossf = loss.item()\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = model.estimate_mfu(batch_size, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea373aa948f4ef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T09:34:26.152466Z",
     "start_time": "2023-12-24T09:34:26.143798Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Test phase begins\n",
    "start = \"Shakespeare \\n\"\n",
    "with open(meta_path, 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "# TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "stoi, itos = meta['stoi'], meta['itos']\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09331f69f4d855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T09:34:33.403619Z",
     "start_time": "2023-12-24T09:34:28.948258Z"
    }
   },
   "outputs": [],
   "source": [
    "#NOTE: Results output by initial models\n",
    "state_dict_init = torch.load(\"./out/ckpt_init.pt\", map_location=device)['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict_init.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict_init[k[len(unwanted_prefix):]] = state_dict_init.pop(k)\n",
    "model.load_state_dict(state_dict_init)\n",
    "with torch.no_grad():\n",
    "        y = model.generate(x, 500, temperature=0.8, top_k=200)\n",
    "        print('------------------------')\n",
    "        print(decode(y[0].tolist()))\n",
    "        print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af58ba50902c424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T09:34:42.188560Z",
     "start_time": "2023-12-24T09:34:37.234397Z"
    }
   },
   "outputs": [],
   "source": [
    "#NOTE: Results output by final models\n",
    "state_dict = torch.load(\"./out/ckpt.pt\", map_location=device)['model']\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "with torch.no_grad():\n",
    "        y = model.generate(x, 500, temperature=0.8, top_k=200)\n",
    "        print('------------------------')\n",
    "        print(decode(y[0].tolist()))\n",
    "        print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbe0a5de4d20af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
