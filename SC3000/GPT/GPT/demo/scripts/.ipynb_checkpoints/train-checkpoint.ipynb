{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12e20073-da90-468d-801d-fd14ec5eb25a",
   "metadata": {},
   "source": [
    "### Stage5: Train our toy GPT model and generative Shakespeare-style texts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc4afd-fa7e-4383-b9c6-8efb1a2914f5",
   "metadata": {},
   "source": [
    "#### (1) Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a12e9-098a-41b3-bdb8-6cc43574a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,pickle,requests,time,math,torch,tiktoken,inspect\n",
    "import numpy as np\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad73fb-9168-4895-a07a-04cfbf47d566",
   "metadata": {},
   "source": [
    "#### (2) Define the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d035de1-d107-40fe-9f18-698d9db633f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        return optimizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059407c7-3070-4c18-a44d-a0d3d3603256",
   "metadata": {},
   "source": [
    "#### (3) Define the batch loader, loss estimator, and learning rate adjustor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ce26f-03d4-4487-a2db-fc6defc6e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51a868-ef95-4aa3-a6b2-06899344e04b",
   "metadata": {},
   "source": [
    "#### (4) Configurate the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63454596d0ae37b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T09:08:47.829709Z",
     "start_time": "2023-12-24T09:08:47.821400Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Configuration\n",
    "dataset = 'shakespeare_char'\n",
    "data_dir = os.path.join('../data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "out_dir = '../out'\n",
    "eval_interval = 200\n",
    "eval_iters = 1\n",
    "batch_size = 12\n",
    "block_size = 64\n",
    "learning_rate = 6e-4\n",
    "max_iters = 5000\n",
    "decay_lr = True\n",
    "warmup_iters = 100\n",
    "lr_decay_iters = max_iters\n",
    "min_lr = learning_rate/10.0\n",
    "tokens_per_iter = batch_size * block_size\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "device = 'mps'\n",
    "device_type = 'cpu'\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype='float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd985f95-39f8-41e3-b714-84385399dbb2",
   "metadata": {},
   "source": [
    "#### (5) Initialize the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d865721de9f8e83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T09:08:49.526872Z",
     "start_time": "2023-12-24T09:08:49.500967Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "model_args = dict(n_layer=4, n_head=4, n_embd=128, block_size=block_size, bias=False, vocab_size=None, dropout=0.0) \n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "model.to(device)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=False)\n",
    "optimizer = model.configure_optimizers(1e-1, learning_rate, (0.9, 0.95), device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ccab7-5c5a-4f65-a1ba-dd1abc62b921",
   "metadata": {},
   "source": [
    "#### (6) The training process begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f0240e70cfe2d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T09:11:14.378530Z",
     "start_time": "2023-12-24T09:08:50.454837Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "iter_list = []\n",
    "\n",
    "if not os.path.exists(out_dir): \n",
    "    print(\"make dir=>{}\".format(out_dir))\n",
    "    os.makedirs(out_dir)\n",
    "t0 = time.time()\n",
    "X, Y = get_batch('train')\n",
    "while True:\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    if iter_num == 0: \n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model_args': model_args,\n",
    "            'iter_num': iter_num,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            }\n",
    "        print(f\"saving initial checkpoint to {out_dir}\")\n",
    "        torch.save(checkpoint, os.path.join(out_dir, 'ckpt_init.pt'))\n",
    "    \n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(iter_num % eval_interval)\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    #for micro_step in range(1):\n",
    "    #with ctx:\n",
    "    logits, loss = model(X, Y)\n",
    "    loss = loss\n",
    "\n",
    "    X, Y = get_batch('train')\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % 1 == 0:\n",
    "        lossf = loss.item()\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
    "    \n",
    "    loss_list.append(lossf)\n",
    "    iter_list.append(iter_num)\n",
    "    \n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6edf44-1413-46b2-b2c6-38939d4b0f72",
   "metadata": {},
   "source": [
    "#### (7) The figure about the loss during the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9891d2e-e80e-4239-ad1a-fea3daac7030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rc('figure', figsize=(12, 4))\n",
    "plt.plot(iter_list, loss_list,c=\"blue\",linewidth=0.8)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid(axis='x', color='grey', linestyle='--', linewidth=0.5)\n",
    "plt.grid(axis='y', color='grey', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b7c80-3cab-4fdf-86a5-47ae417d6e9d",
   "metadata": {},
   "source": [
    "#### (8) Generate Shakespeare-style texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be14e40-4457-4565-89e4-e3aba5d52ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"Shakespeare style \\n\"\n",
    "with open(meta_path, 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "# TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "stoi, itos = meta['stoi'], meta['itos']\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0fe6a3-24ba-48e9-bbbb-26b8b605cc7e",
   "metadata": {},
   "source": [
    "##### --> The results generated at iter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed9252-a265-4a50-9a71-7e0687ac6a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_init = torch.load(\"../out/ckpt_init.pt\", map_location=device)['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict_init.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict_init[k[len(unwanted_prefix):]] = state_dict_init.pop(k)\n",
    "model.load_state_dict(state_dict_init)\n",
    "with torch.no_grad():\n",
    "        y = model.generate(x, 500, temperature=0.8, top_k=200)\n",
    "        print('------------------------')\n",
    "        print(decode(y[0].tolist()))\n",
    "        print('------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c39404-2b06-4521-bd83-fef33d8ebb10",
   "metadata": {},
   "source": [
    "##### --> The results generated at iter=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e0acfe-41bb-4ee1-991b-8dc912fa1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../out/ckpt.pt\", map_location=device)['model']\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "with torch.no_grad():\n",
    "        y = model.generate(x, 500, temperature=0.8, top_k=200)\n",
    "        print('------------------------')\n",
    "        print(decode(y[0].tolist()))\n",
    "        print('------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
